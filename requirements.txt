transformers==4.57.1
flash-attn==2.7.3 # or download from https://github.com/Dao-AILab/flash-attention/releases?page=1 and choose ABI false.
deepspeed
accelerate
torchcodec
black
isort
datasets
evaluate
httpx
jsonlines
numpy
peft
pytablewriter
sacrebleu
sqlitedict
timm
ftfy
hf_transfer
nltk
yt-dlp
pycocoevalcap
tqdm-multiprocess
transformers-stream-generator
zstandard
openpyxl
loguru
tenacity
wandb
pre-commit
decord
zss
scipy
matplotlib
protobuf
qwen_vl_utils
spicy
terminaltables

